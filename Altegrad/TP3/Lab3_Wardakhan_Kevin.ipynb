{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcayLcTltUeT"
      },
      "source": [
        "<center>\n",
        "<h1>\n",
        "<h1>APM 53674: ALTeGraD</h1>\n",
        "<h2>Lab Session 3: Improving LLMs with RLHF (DPO & GRPO)</h2>\n",
        "<h4>Lecture: Prof. Michalis Vazirgiannis<br>\n",
        "Lab: Yang Zhang and Xiao Fei</h4>\n",
        "<h5>Tuesday, October 14, 2025</h5>\n",
        "<br>\n",
        "</center>\n",
        "\n",
        "<hr style=\"border:10px solid gray\"> </hr>\n",
        "<p style=\"text-align: justify;\">\n",
        "This handout includes theoretical introductions, <font color='blue'>coding tasks</font> and <font color='red'>questions</font>. Before the deadline, you should submit <a href='https://forms.gle/9dyaes6dimfvyjwq6' target=\"_blank\">here</a> a <B>.ipynb</B> file named <b>Lastname_Firstname.ipynb</b> containing your notebook (with the gaps filled and your answers to the questions). Your answers should be well constructed and well justified. They should not repeat the question or generalities in the handout. When relevant, you are welcome to include figures, equations and tables derived from your own computations, theoretical proofs or qualitative explanations. One submission is required for each student. The deadline for this lab is <b>October 19\n",
        ", 2025 11:59 PM</b>. No extension will be granted. Late policy is as follows: ]0, 24] hours late ‚Üí -5 pts; ]24, 48] hours late ‚Üí -10 pts; > 48 hours late ‚Üí not graded (zero).\n",
        "</p>\n",
        "<hr style=\"border:5px solid gray\"> </hr>\n",
        "\n",
        "\n",
        "# ü§ñ Post-Training: Improving LLMs with RLHF (DPO & GRPO)\n",
        "\n",
        "In this notebook, we‚Äôll show how to improve a language model using **two post-training techniques**:\n",
        "\n",
        "### üß† What You‚Äôll Learn\n",
        "- What **Direct Preference Optimization (DPO)** is and how it helps models choose better answers.\n",
        "- What **Group Relative Policy Optimization (GRPO)** is and how it helps models solve tasks  improving reasoning and performance on complex tasks (math, code, logic).\n",
        "- How to train small models on **real feedback data**.\n",
        "- How to **observe changes in model behavior** after fine-tuning.\n",
        "\n",
        "\n",
        "### üì¶ What We‚Äôll Use\n",
        "- **Hugging Face ü§ó Transformers** to load and run models\n",
        "- **TRL (Transformer Reinforcement Learning)** library by Hugging Face ü§ó for DPO and GRPO\n",
        "- **A small version** of the French translated [Anthropic HH-RLHF dataset](https://huggingface.co/datasets/AIffl/french_hh_rlhf)\n",
        "- **Colab GPU**, so models are small enough to run quickly\n",
        "\n",
        "### Useful links:\n",
        "- [Hugging Face ü§ó DPO Trainer](https://huggingface.co/docs/trl/dpo_trainer)\n",
        "- [Hugging Face ü§ó GRPO Trainer](https://huggingface.co/docs/trl/grpo_trainer)\n",
        "\n",
        "> This notebook is interactive, friendly, and high-level. You don‚Äôt need to know deep math or theory to follow along."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wH0u4hhtUeV"
      },
      "source": [
        "# Quick start:\n",
        "1-  Clone the repository:\n",
        "```bash\n",
        "git clone https://github.com/BounharAbdelaziz/RLHF.git\n",
        "```\n",
        "2- Install the dependencies:\n",
        "```bash\n",
        "pip install -q -r requirements.txt\n",
        "```\n",
        "Now we are ready to go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7EtCgK2t1lf",
        "outputId": "1d4840c4-15f8-49c3-d94d-9a0b04cdcaf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPEWV5M_tUeV",
        "outputId": "ba614b1e-38bf-4148-ec7f-b636b8f5d88b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'RLHF' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/BounharAbdelaziz/RLHF.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAXzTCmJtX3s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e07bb002-c605-4c68-ca84-20ea04eab459"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -r RLHF/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A8MAR6WtUeV"
      },
      "source": [
        "# Part I: DPO\n",
        "\n",
        "# üß† Fine-tuning Qwen2.5-0.5B-Instruct on French Data\n",
        "\n",
        "In this part, we‚Äôll walk through how to **fine-tune the Qwen2.5-0.5B-Instruct** model on **French-language data**, using **off-policy DPO (Direct Preference Optimization)**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Key Concepts\n",
        "\n",
        "- **Model**: [Qwen2.5-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct)  \n",
        "- **Objective**: Adapt the model for French understanding and instruction-following  \n",
        "- **Method**: Off-policy **DPO** for alignment-based fine-tuning  \n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è System Requirements\n",
        "\n",
        "Before training, make sure you choose an appropriate GPU setup.\n",
        "\n",
        "- **Memory requirements**: [GPU memory guidance](https://rahulschand.github.io/gpu_poor/)  \n",
        "- **GPU options**:\n",
        "  - AWS SageMaker: [pricing calculator](https://aws.amazon.com/sagemaker-ai/pricing/)  \n",
        "  - üí∏ Cheaper alternative: [RunPod](https://www.runpod.io/)  \n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ Memory Optimization\n",
        "\n",
        "We‚Äôll use **LoRA** (Low-Rank Adaptation) combined with **quantization (4-bit)** to reduce GPU memory usage while maintaining performance.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Evaluation Tools\n",
        "\n",
        "To evaluate the fine-tuned model, you can use one or more of the following frameworks:\n",
        "\n",
        "- [`lm-eval-harness`](https://github.com/EleutherAI/lm-evaluation-harness)\n",
        "- [`lighteval`](https://github.com/huggingface/evaluate)\n",
        "- [`libra-eval`](https://github.com/facebookresearch/libra)\n",
        "\n",
        "---\n",
        "Below is a visual overview of the **Direct Preference Optimization (DPO)** training process:\n",
        "\n",
        "![DPO Training Diagram](https://1drv.ms/i/c/ae69638675180117/IQQ_wS77RKdrS4tzLbwoqr1gAR0Bf_1X2U36NRBp1Odsypg?width=560&height=48)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8AyeUIktUeV"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from trl import (\n",
        "    DPOTrainer,\n",
        "    DPOConfig,\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        ")\n",
        "import torch\n",
        "import os\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2czRsmsAQEF4"
      },
      "source": [
        "## üìä Tracking with Weights & Biases (W&B)\n",
        "\n",
        "We‚Äôll use **Weights & Biases (W&B)** to log training metrics, model versions, and system stats so you can compare runs, debug faster, and share results. Before running the next cell, **create a free account** at [https://wandb.ai](https://wandb.ai) and make a new **Project** (e.g., `RLHF`). In the code cell that follows, we‚Äôll initialize W&B; on first use you‚Äôll need to **register** then paste your **API key** from your W&B profile. During training, W&B will automatically track losses, learning rate, gradient norms, and GPU utilization, and we‚Äôll log custom metrics (e.g., validation perplexity, evaluation scores) plus configuration details (dataset, hyperparameters, LoRA/quantization settings). Each run will appear on your project dashboard with charts, tables, run metadata, and artifacts, making it easy to **compare experiments**, **resume runs**, and **share dashboards** with your teammates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vtHPrUXYYVO"
      },
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 1: </b><br>\n",
        "Create your Weights&Biases account and fill the gap the next cell.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6J6EMxdtUeV"
      },
      "outputs": [],
      "source": [
        "# We will use wandb.ai for logging the experiments - Set your API key here\n",
        "WANDB_API_KEY = \"3eed34e1dab8e3cbcfc99646e97521cac9709865\" # fill the gap with your wandb account\n",
        "\n",
        "# Automatically login using the API key\n",
        "os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
        "os.environ[\"WANDB_PROJECT\"] = \"RLHF\"\n",
        "wandb.login()\n",
        "\n",
        "# Training dataset\n",
        "DATASET_PATH = \"AIffl/french_orca_dpo_pairs\" # french version of \"Intel/orca_dpo_pairs\"\n",
        "\n",
        "# We limit to 2k samples for speed\n",
        "LIMIT = 2_000\n",
        "\n",
        "# SFT Model we will finetune\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "# Seed for reproducibility\n",
        "SEED = 1998\n",
        "\n",
        "MAX_PROMPT_LEN = 1024\n",
        "MAX_LENGTH = MAX_PROMPT_LEN + 512\n",
        "\n",
        "RUN_NAME = \"DPO-french-orca-\" + MODEL_NAME.split('/')[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS_Fgnc4tUeW"
      },
      "source": [
        "## Load the SFT Model and Tokenizer\n",
        "\n",
        "We‚Äôll stick with 4-bit quantization via bitsandbytes for this lab. You‚Äôve already used it last week, so nothing new‚Äîsame setup (load the model with 4-bit weights), same goal (reduce VRAM) with minimal impact on quality for our use case. This keeps runs feasible on a single GPU.\n",
        "\n",
        "We will need the tonkenizer in the data preparation step to apply the chat template."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzt-X_0mYnEl"
      },
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 2: </b><br>\n",
        "Create the quantization confiduration to load the model with 4 bits\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx0a87p1tUeW"
      },
      "outputs": [],
      "source": [
        "# Quantization configuration\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype= torch.bfloat16, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True)\n",
        "\n",
        "# Load the model to finetune\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "# to avoid warning\n",
        "model.config.use_cache = False\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "# Set padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Wb1mjaltUeW"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "### üí¨ Chat templates & converting data to `messages`\n",
        "\n",
        "Modern instruction-tuned models (including **Qwen2.5-0.5B-Instruct**) expect inputs in a **chat format** and rely on a **tokenizer chat template** to turn structured messages into the exact token sequence the model was trained on. In practice, you should **not** hand-craft special tokens; instead, pass a list of `{role, content}` messages to the tokenizer and let `apply_chat_template(...)` do the right thing.\n",
        "\n",
        "#### Why a chat template?\n",
        "- Ensures your prompts match the **pretraining/finetuning format** (system/user/assistant turns, BOS/EOS, separators).\n",
        "- Minimizes prompt drift across libraries and models.\n",
        "- Makes it easy to add **system instructions** (e.g., ‚ÄúYou are a helpful assistant that answers in French.‚Äù).\n",
        "\n",
        "#### Message structure\n",
        "Each example becomes an ordered list of chat turns:\n",
        "```python\n",
        "messages = [\n",
        "  {\"role\": \"system\", \"content\": \"Tu es un assistant utile. R√©ponds en fran√ßais.\"},\n",
        "  {\"role\": \"user\", \"content\": \"Explique la diff√©rence entre LoRA et le fine-tuning complet.\"},\n",
        "  {\"role\": \"assistant\", \"content\": \"LoRA adapte un petit sous-espace de poids, alors que...\"}\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3h1j7ezZL4R"
      },
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 3: </b><br>\n",
        "Create the user message which is the question field of the dataset.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oW5vfRm0tUeW"
      },
      "outputs": [],
      "source": [
        "def preprocess_for_dpo(example):\n",
        "    # Format system message if present\n",
        "    messages = []\n",
        "    if example.get('system') and len(example['system'].strip()) > 0:\n",
        "        messages.append({\"role\": \"system\", \"content\": example['system']})\n",
        "\n",
        "    # fill the gap, add user message\n",
        "    user_message = {\"role\": \"user\", \"content\": example.get('question')}\n",
        "    messages.append(user_message)\n",
        "\n",
        "    # Create prompt with generation prompt for DPO\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # The chosen and rejected should be the assistant responses only\n",
        "    chosen = example['chosen']\n",
        "    rejected = example['rejected']\n",
        "\n",
        "    return {\n",
        "        \"prompt\": prompt,\n",
        "        \"chosen\": chosen,\n",
        "        \"rejected\": rejected,\n",
        "    }\n",
        "\n",
        "# Download the training dataset\n",
        "dataset = load_dataset(DATASET_PATH, split=f\"train\")\n",
        "# shuffle and select a number of samples\n",
        "dataset = dataset.shuffle(True).select(range(LIMIT))\n",
        "\n",
        "# Save columns\n",
        "original_columns = dataset.column_names\n",
        "\n",
        "# Apply the preprocessing function\n",
        "dpo_dataset = dataset.map(\n",
        "    preprocess_for_dpo,\n",
        "    remove_columns=original_columns,\n",
        ")\n",
        "\n",
        "# Filter out examples that are too long\n",
        "def filter_length(example):\n",
        "    prompt_length = len(tokenizer.encode(example['prompt']))\n",
        "    chosen_length = len(tokenizer.encode(example['chosen']))\n",
        "    rejected_length = len(tokenizer.encode(example['rejected']))\n",
        "\n",
        "    return (prompt_length + max(chosen_length, rejected_length)) < MAX_LENGTH\n",
        "\n",
        "dpo_dataset = dpo_dataset.filter(filter_length)\n",
        "\n",
        "print(f\"Dataset size after filtering: {len(dpo_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Mt2KOACyNOh"
      },
      "outputs": [],
      "source": [
        "display(dpo_dataset.column_names)\n",
        "display(dpo_dataset.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75nrGIGktUeW"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "Training will mirror last week‚Äôs lab, but we‚Äôll switch from SFT to **off-policy DPO** using the `trl` library. Concretely, we‚Äôll instantiate a **policy model** (trainable) and a **reference model** (frozen) and optimize with the DPO objective so the policy prefers **chosen** over **rejected** responses for the same prompt.\n",
        "\n",
        "### What we‚Äôll use\n",
        "- **TRL**: `DPOConfig`, `DPOTrainer`\n",
        "- **PEFT**: LoRA adapters on top of the base **Qwen2.5-0.5B-Instruct**\n",
        "- **Quantization**: 4-bit (QLoRA-style) to fit on small GPUs\n",
        "- **Logging**: W&B for metrics, configs, and artifacts\n",
        "\n",
        "### Expected dataset columns\n",
        "- `prompt` (or `messages`): the shared context (system+user turns)\n",
        "- `chosen`: assistant reply preferred by annotators\n",
        "- `rejected`: less-preferred reply\n",
        "> If you‚Äôre keeping everything in chat format, we‚Äôll pass lists of `{role, content}` and rely on `tokenizer.apply_chat_template(...)` inside the collator.\n",
        "\n",
        "### Minimal training flow\n",
        "1. Load tokenizer with the **chat template** and enable 4-bit loading of the base model.\n",
        "2. Wrap the model with **LoRA** (target attention/MLP modules).\n",
        "3. Build a `datasets.Dataset` that yields `(prompt/messages, chosen, rejected)`.\n",
        "4. Define `DPOConfig` (batch size, lr, epochs, `beta`, logging/saving/eval cadence).\n",
        "5. Create `DPOTrainer(policy_model, ref_model, tokenizer, train_dataset, eval_dataset, **config)`.\n",
        "6. Call `trainer.train()`; optional `trainer.evaluate()` and `trainer.save_model()`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNKXz-pbaj0l"
      },
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 4: </b><br>\n",
        "Create the lora config with rank of 32, alpha of 64 and dropout of 0.1 on all MLP layers (execluding Embedding layers) and train the model\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJeBW-l6tUeW",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# LoRA configuration - targeting the correct modules for Qwen2.5\n",
        "peft_config = LoraConfig(r=32, lora_alpha=64, lora_dropout=0.1, target_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], bias=\"none\", task_type = 'CAUSAL_LM')\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Training configuration\n",
        "training_args = DPOConfig(\n",
        "    beta=0.1,  # DPO temperature parameter\n",
        "    learning_rate=5e-6,\n",
        "    max_prompt_length=MAX_PROMPT_LEN,\n",
        "    max_length=MAX_LENGTH,\n",
        "    per_device_train_batch_size=1,  # Reduced for memory\n",
        "    gradient_accumulation_steps=4,  # Increased to maintain effective batch size of 4 (1*4)\n",
        "    num_train_epochs=1,\n",
        "    max_grad_norm=1.0,\n",
        "    logging_steps=1,\n",
        "    save_steps=100,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"paged_adamw_8bit\",  # More memory efficient\n",
        "    warmup_ratio=0.03, # 3% of the steps will be just a warmup\n",
        "    save_strategy=\"steps\",\n",
        "    output_dir=\"./dpo_model\",\n",
        "    report_to=\"wandb\",\n",
        "    run_name=RUN_NAME,\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_pin_memory=False,\n",
        "    fp16=True,  # Enable mixed precision\n",
        ")\n",
        "\n",
        "# Initialize the trainer - Note: no ref_model needed when using peft_config\n",
        "trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    peft_config=peft_config,  # This automatically handles reference model\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=dpo_dataset,\n",
        ")\n",
        "\n",
        "# Print a sample to verify preprocessing\n",
        "print(\"Sample from dataset:\")\n",
        "print(f\"Prompt: {dpo_dataset[0]['prompt']}\")\n",
        "print(f\"Chosen: {dpo_dataset[0]['chosen']}\")\n",
        "print(f\"Rejected: {dpo_dataset[0]['rejected']}\")\n",
        "\n",
        "# Train\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mm3zJDWVtUeW"
      },
      "outputs": [],
      "source": [
        "# merge LoRA adapters with the base model\n",
        "save_path = \"dpo_model/final_merged_dpo_model\"\n",
        "\n",
        "model = model.merge_and_unload()\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkbrN3GltUeW"
      },
      "source": [
        "## Model Testing\n",
        "\n",
        "We will test the DPO model via chat-app\n",
        "To validate alignment gains, we‚Äôll spin up a small **Gradio** app that queries both the **pre-DPO** model (baseline/reference) and the **post-DPO** **policy** side-by-side. The UI lets you enter a French prompt, then compares generations using the **same chat template** and decoding settings. This helps spot qualitative shifts in helpfulness, safety, and instruction-following."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKtCZEoxtUeW"
      },
      "outputs": [],
      "source": [
        "from RLHF.chat_app import launch_chat_app\n",
        "\n",
        "launch_chat_app(\n",
        "    model_path=save_path, #\"habdine/CSC_53432_lab3_dpo\",\n",
        "    base_model_path=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "    title=\"ü§ñ Dual-Model Qwen Chat (DPO vs Base) for French\",\n",
        "    DPO_TEST=True,\n",
        "    FRENCH_TEST=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvmpsUrOtUeX"
      },
      "outputs": [],
      "source": [
        "from RLHF.chat_app import launch_chat_app\n",
        "\n",
        "launch_chat_app(\n",
        "    model_path=\"BounharAbdelaziz/Qwen2.5-0.5B-DPO-English-Orca\",\n",
        "    base_model_path=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "    title=\"ü§ñ Dual-Model Qwen Chat (DPO vs Base) for English\",\n",
        "    DPO_TEST=True,\n",
        "    FRENCH_TEST=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb2KC8TJtUeX"
      },
      "source": [
        "# Part II: GRPO\n",
        "\n",
        "\n",
        "The following diagram illustrates the **GRPO (Generative Reinforcement Preference Optimization)** process ‚Äî an alternative to DPO that directly optimizes generation quality from preference data using reinforcement-style updates:\n",
        "\n",
        "![GRPO Training Overview](https://1drv.ms/i/c/ae69638675180117/IQQ-KizPdUxCRZU9qDGcpX1AAeettH1uhsJqqM1WjXiYR6s?width=705&height=66)\n",
        "\n",
        "After exploring DPO, we now move on to **GRPO** ‚Äî a reinforcement learning‚Äìstyle approach that builds directly on preference data.  \n",
        "While **DPO** adjusts the model using an *analytic loss* derived from preference pairs, **GRPO** takes a more dynamic route: it uses **reward modeling and policy gradients** to optimize the model through sampled generations.\n",
        "\n",
        "In essence:\n",
        "- GRPO **learns from human (or model) preferences** using *on-policy* updates.  \n",
        "- It combines elements of **PPO** (Proximal Policy Optimization) with **preference-based rewards** rather than explicit numerical scores.  \n",
        "- This allows the model to better capture *generation quality* aspects that aren‚Äôt directly expressible through static loss terms.\n",
        "\n",
        "In the next section, we‚Äôll explore how to configure and launch a **GRPO training loop** using `trl`, reusing much of our previous setup (tokenizer, LoRA, quantization) but switching to **on-policy optimization**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9K2jcjn12YY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    TaskType,\n",
        "    prepare_model_for_kbit_training,\n",
        ")\n",
        "from trl import (\n",
        "    GRPOConfig,\n",
        "    GRPOTrainer,\n",
        ")\n",
        "import os\n",
        "import wandb\n",
        "\n",
        "# We will use wandb.ai for logging the experiments - Set your API key here\n",
        "WANDB_API_KEY = \"3eed34e1dab8e3cbcfc99646e97521cac9709865\" # fill the gap with your wandb key\n",
        "\n",
        "# Automatically login using the API key\n",
        "os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
        "os.environ[\"WANDB_PROJECT\"] = \"RLHF\"\n",
        "wandb.login()\n",
        "\n",
        "# Training dataset\n",
        "DATASET_PATH = \"openai/gsm8k\"\n",
        "\n",
        "# We limit to 200 samples for speed\n",
        "LIMIT = 200\n",
        "\n",
        "# SFT Model we will finetune\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "# MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "# Seed for reproducibility\n",
        "SEED = 1998\n",
        "\n",
        "USE_LORA = True\n",
        "\n",
        "if MODEL_NAME == \"Qwen/Qwen2.5-0.5B-Instruct\":\n",
        "  USE_QUANT = False\n",
        "else:\n",
        "  USE_QUANT = True\n",
        "\n",
        "lora_alpha = 128\n",
        "lora_r = 64\n",
        "lora_dropout = 0.1\n",
        "\n",
        "if not USE_LORA:\n",
        "  MAX_PROMPT_LEN = 512\n",
        "  MAX_LENGTH = MAX_PROMPT_LEN + 512\n",
        "else:\n",
        "    MAX_PROMPT_LEN = 150\n",
        "    MAX_LENGTH = MAX_PROMPT_LEN + 150\n",
        "\n",
        "RUN_NAME = \"GRPO-GSM8K-limit-\" + str(LIMIT) + \"-\" + MODEL_NAME.split('/')[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fRTvtVGtUeX"
      },
      "source": [
        "## Load the SFT Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UkT_7Y_tUeX"
      },
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Quantization configuration\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load the model to finetune\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    quantization_config=quantization_config if USE_QUANT else None,\n",
        "    device_map=\"cuda:0\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "if USE_QUANT:\n",
        "  model = prepare_model_for_kbit_training(model)\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Add padding token if not exists\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "if USE_LORA:\n",
        "  # Configure LoRA\n",
        "  lora_config = LoraConfig(\n",
        "      r=lora_r,  # Rank of adaptation : 64\n",
        "      lora_alpha=lora_alpha,  # LoRA scaling parameter : 128\n",
        "      target_modules=[\n",
        "          \"q_proj\",\n",
        "          \"k_proj\",\n",
        "          \"v_proj\",\n",
        "          \"o_proj\",\n",
        "          \"gate_proj\",\n",
        "          \"up_proj\",\n",
        "          \"down_proj\",\n",
        "      ],  # Target modules for Qwen2.5 architecture\n",
        "      lora_dropout=lora_dropout,  # LoRA dropout : 0.1\n",
        "      bias=\"none\",  # Bias type\n",
        "      task_type=TaskType.CAUSAL_LM,  # Task type\n",
        "  )\n",
        "\n",
        "  # Apply LoRA to the model\n",
        "  model = get_peft_model(model, lora_config)\n",
        "\n",
        "  # Print trainable parameters\n",
        "  model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jo1slG9tUeX"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "For GRPO training, we‚Äôll use the **GSM8K** dataset ‚Äî a benchmark of grade-school math word problems.  \n",
        "Each problem includes a **question** and a **final answer**. Our goal is to teach the model to reason in French (or English if you prefer) and **output only the final numeric answer** enclosed between `<answer>` and `</answer>` tags.\n",
        "\n",
        "This format makes automatic evaluation trivial ‚Äî we can extract the number between tags and compare it directly to the reference.\n",
        "\n",
        "With GRPO the model will learn two main things:\n",
        "- How follow the instruction to output the correct format.\n",
        "- Gain more math capabilities\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Why structure as chat messages?\n",
        "\n",
        "Just like with DPO, the **Qwen2.5-0.5B-Instruct** model expects inputs in a *chat-style message format*.  \n",
        "We‚Äôll use:\n",
        "- A **system message** to define the task and output style.\n",
        "- A **user message** with the math problem.\n",
        "- An **assistant message** containing the reasoning and final numeric answer wrapped in tags."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DQ-3KLwb242"
      },
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 5: </b><br>\n",
        "Create your training samples as following:<br>\n",
        "1- system message: R1_STYLE_SYSTEM_PROMPT + \"\\n\" + TASK_SPECIFIC_INSTRUCTIONS <br>\n",
        "2- one-shot example of user message of \"What is 2+2?\" and an assistant message of \"To calculate 2+2, we simply add the numbers together: 2 + 2 = 4.\\n$<answer>$4$</answer>$\" <br>\n",
        "3- the question sample from the dataset as a user message.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U95rr5j-tUeX"
      },
      "outputs": [],
      "source": [
        "# Load GSM8K dataset\n",
        "if LIMIT:\n",
        "    dataset = load_dataset(DATASET_PATH, \"main\", split=f\"train[:{LIMIT}]\")  # Small subset for demo\n",
        "else:\n",
        "    dataset = load_dataset(DATASET_PATH, \"main\")\n",
        "\n",
        "R1_STYLE_SYSTEM_PROMPT = \"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n",
        "The assistant first thinks shortly about the reasoning process in the mind and then provides the user with the answer in a new line between <answer> and </answer>.\"\"\"\n",
        "\n",
        "TASK_SPECIFIC_INSTRUCTIONS = \"The answer must be a single integer.\"\n",
        "# print(dataset.column_names)\n",
        "# print(dataset[0])\n",
        "def preprocess_dataset(dataset, chunk_size=1000) -> Dataset:\n",
        "\n",
        "    def extract_hash_answer(text: str) -> str | None:\n",
        "        try:\n",
        "            return text.split(\"####\")[1].strip()\n",
        "        except IndexError:\n",
        "            return None\n",
        "\n",
        "    def process_batch(batch):\n",
        "        prompts = []\n",
        "        for i in range(len(batch['question'])):\n",
        "            messages = [\n",
        "                {'role': 'system', 'content': R1_STYLE_SYSTEM_PROMPT + \"\\n\" + TASK_SPECIFIC_INSTRUCTIONS},\n",
        "                # One-shot example\n",
        "                {'role': 'user', 'content': \"What is 2+2?\"},\n",
        "                {'role': 'assistant', 'content': \"To calculate 2+2, we simply add the numbers together: 2 + 2 = 4.\\n<answer>4</answer>\"},\n",
        "                # Task-specific user message\n",
        "                {'role': 'user', 'content': batch['question'][i].strip()}\n",
        "            ]\n",
        "            prompts.append(messages)\n",
        "\n",
        "        return {\n",
        "            'prompt': prompts,\n",
        "            'answer': [extract_hash_answer(a) for a in batch['answer']]\n",
        "        }\n",
        "\n",
        "    return dataset.map(process_batch, batched=True, batch_size=chunk_size)\n",
        "train_dataset = preprocess_dataset(dataset, chunk_size=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvwShnXPhJxI"
      },
      "outputs": [],
      "source": [
        "print(train_dataset.column_names)\n",
        "\n",
        "print(train_dataset[0]['answer'])\n",
        "display(train_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AsDSYi3tUeX"
      },
      "source": [
        "## Reward Function Design (2)\n",
        "\n",
        "We‚Äôll use **two simple rewards** during GRPO rollouts:\n",
        "\n",
        "1. **Format reward** ‚Äî checks that the **last non-empty line** is exactly in the form  \n",
        "   `<answer>NUMBER</answer>`  \n",
        "   - Score: **1** if correct format, **0** otherwise.\n",
        "\n",
        "2. **Correctness reward** ‚Äî checks whether the extracted number matches the gold answer.  \n",
        "   - Score: **2** if correct, **0** otherwise.\n",
        "\n",
        "Total reward per sample ‚àà {0, 1, 2, 3}.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sLEkm7ec9HN"
      },
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 6: </b><br>\n",
        "write the `extract_xml_answer` function to extract the answer from the generated text.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjvdHWcVtUeX"
      },
      "outputs": [],
      "source": [
        "def extract_xml_answer(text: str) -> str:\n",
        "    answer = text[text.find(\"<answer>\")+len(\"<answer>\"):text.find(\"</answer>\")]\n",
        "    return answer\n",
        "\n",
        "def format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has the correct format.\"\"\"\n",
        "    pattern = r\"^(?:[^\\r\\n]*\\r?\\n)+<answer>\\d+</answer>\\r?\\n?$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [bool(re.match(pattern, r)) for r in responses]\n",
        "    return [1.0 if match else 0.0 for match in matches]\n",
        "\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the answer is correct.\"\"\"\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoWWcLFhtUeX"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAcuDydttUeY"
      },
      "outputs": [],
      "source": [
        "# GRPO Configuration\n",
        "grpo_config = GRPOConfig(\n",
        "    output_dir=\"./grpo_model\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_train_epochs=3,\n",
        "    max_prompt_length=MAX_PROMPT_LEN,\n",
        "    max_completion_length=MAX_LENGTH,\n",
        "    num_generations=2, # The effective train batch size must be evenly divisible by the number of generations per prompt\n",
        "    beta=0,\n",
        "    epsilon=0.28,\n",
        "    temperature=1,\n",
        "    logging_steps=1,\n",
        "    save_steps=25,\n",
        "    save_total_limit=3,\n",
        "    # load_best_model_at_end=True,\n",
        "    # metric_for_best_model=\"reward\",\n",
        "    # greater_is_better=True,\n",
        "    run_name=RUN_NAME,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"paged_adamw_8bit\" ,  # More memory efficient\n",
        "    warmup_ratio=0.03, # 3% of the steps will be just a warmup\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_pin_memory=False,\n",
        "    fp16=True,  # Enable mixed precision\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    reward_funcs=[format_reward_func, correctness_reward_func],\n",
        "    args=grpo_config,\n",
        "    train_dataset=train_dataset,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "# Training\n",
        "print(\"Starting GRPO training...\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vt3VtLn0tUeY"
      },
      "outputs": [],
      "source": [
        "# merge LoRA adapters with the base model\n",
        "save_path = \"grpo_model/final_merged_grpo_model\"\n",
        "\n",
        "model = model.merge_and_unload()\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWgS802UtUeY"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ViMeNJ9tUeY"
      },
      "outputs": [],
      "source": [
        "from RLHF.chat_app import launch_chat_app\n",
        "\n",
        "launch_chat_app(\n",
        "    model_path=save_path,#\"habdine/CSC_53432_lab3_grpo\"\n",
        "    base_model_path=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "    title=\"ü§ñ Dual-Model Qwen Chat (GRPO vs Base) for Math\",\n",
        "    DPO_TEST=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIpnugPkeStI"
      },
      "source": [
        "## üìà Model Evaluation\n",
        "\n",
        "Once our GRPO-trained model is ready, we need to **evaluate its performance**  ‚Äî to verify that it has learned to produce correctly formatted and accurate answers.\n",
        "\n",
        "For computational issues, we will evaluate on the first 200 samples only.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEE-nS0I33TW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "import json\n",
        "from datetime import datetime\n",
        "import logging\n",
        "\n",
        "\n",
        "def extract_hash_answer(text: str) -> str | None:\n",
        "    try:\n",
        "        return text.split(\"####\")[1].strip()\n",
        "    except IndexError:\n",
        "        return None\n",
        "\n",
        "def evaluate_model(\n",
        "    model_path: str,\n",
        "    batch_size: int = 1,\n",
        "    num_samples: int = None,\n",
        "    save_results: bool = True,\n",
        ") -> Dict:\n",
        "    print(\"Initializing evaluation...\")\n",
        "\n",
        "    with tqdm(total=2, desc=\"Loading model components\") as pbar:\n",
        "        llm = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"cuda:0\",\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "        pbar.update(1)\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_path,\n",
        "            model_max_length=768,\n",
        "        )\n",
        "        pbar.update(1)\n",
        "\n",
        "    # Load test dataset\n",
        "    print(\"Loading dataset...\")\n",
        "    dataset = load_dataset('openai/gsm8k', 'main', split='test')\n",
        "    if num_samples:\n",
        "        dataset = dataset.select(range(num_samples))\n",
        "    total_samples = len(dataset)\n",
        "    print(f\"Loaded {total_samples} samples\")\n",
        "\n",
        "    results = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Create progress bar\n",
        "    progress_bar = tqdm(\n",
        "        total=total_samples,\n",
        "        desc=\"Processing samples\",\n",
        "        unit=\"examples\",\n",
        "        dynamic_ncols=True,\n",
        "    )\n",
        "\n",
        "    progress_bar.set_postfix({\n",
        "        'acc': '0.00%',\n",
        "        'correct': '0',\n",
        "    })\n",
        "\n",
        "    # Process in batches\n",
        "    for i in range(0, total_samples, batch_size):\n",
        "        batch_data = dataset[i:i + batch_size]\n",
        "        current_batch_size = len(batch_data['question'])\n",
        "\n",
        "        # Prepare prompts using same format as training\n",
        "        prompts = [\n",
        "            [\n",
        "                {'role': 'system', 'content': R1_STYLE_SYSTEM_PROMPT + \"\\n\" + TASK_SPECIFIC_INSTRUCTIONS},\n",
        "                {'role': 'user', 'content': \"What is 2+2?\"},\n",
        "                {'role': 'assistant', 'content': \"To calculate 2+2, we simply add the numbers together: 2 + 2 = 4.\\n<answer>4</answer>\"},\n",
        "                {'role': 'user', 'content': q.strip()}\n",
        "            ] for q in batch_data['question']\n",
        "        ]\n",
        "\n",
        "        # Convert to chat format\n",
        "        formatted_prompts = [\n",
        "            tokenizer.apply_chat_template(\n",
        "                p,\n",
        "                tokenize=True,\n",
        "                return_tensors='pt',\n",
        "                add_generation_prompt=True\n",
        "            )\n",
        "            for p in prompts\n",
        "        ]\n",
        "\n",
        "        # Generate responses\n",
        "        outputs = []\n",
        "        for prompt in formatted_prompts:\n",
        "            output = llm.generate(\n",
        "                prompt.to('cuda:0'),\n",
        "                max_new_tokens=512,\n",
        "                temperature=1.0,\n",
        "            )\n",
        "            outputs.append(output)\n",
        "\n",
        "\n",
        "        # Process responses\n",
        "        for j, output in enumerate(outputs):\n",
        "\n",
        "            response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Extract answers\n",
        "            generated_answer = extract_xml_answer(response)\n",
        "            true_answer = extract_hash_answer(batch_data['answer'][j])\n",
        "\n",
        "            # Store result\n",
        "            result = {\n",
        "                'question': batch_data['question'][j],\n",
        "                'true_answer': true_answer,\n",
        "                'generated_answer': generated_answer,\n",
        "                'full_response': response,\n",
        "                'correct': generated_answer == true_answer\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "            # Update metrics\n",
        "            if generated_answer == true_answer:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "        # Update progress\n",
        "        progress_bar.update(current_batch_size)\n",
        "        progress_bar.set_postfix({\n",
        "            'acc': f'{(correct/total)*100:.2f}%',\n",
        "            'correct': f'{correct}/{total}',\n",
        "        })\n",
        "\n",
        "    progress_bar.close()\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'correct': correct,\n",
        "        'total': total,\n",
        "        'model_path': model_path,\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # Save results\n",
        "    if save_results:\n",
        "        save_path = f\"gsm8k_eval_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "        with open(save_path, 'w') as f:\n",
        "            json.dump({\n",
        "                'metrics': metrics,\n",
        "                'results': results\n",
        "            }, f, indent=2)\n",
        "        print(f\"\\nResults saved to {save_path}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "print(\"Starting GSM8K evaluation...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcED_4Npdms2"
      },
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 7: </b><br>\n",
        "evaluate the model before and after GRPO training.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqjBrw25ehbd"
      },
      "source": [
        "### Evaluation after GRPO\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVBttE_aei4B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "da69d84b-a92b-481c-bd1b-93c87ef593f6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'evaluate_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-98008340.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgrpo_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/grpo_model/final_merged_grpo_model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrpo_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLIMIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'evaluate_model' is not defined"
          ]
        }
      ],
      "source": [
        "grpo_file_path = \"/content/grpo_model/final_merged_grpo_model\"\n",
        "metrics = evaluate_model(grpo_file_path, num_samples=LIMIT)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_grpo =\"/content/gsm8k_eval_results_20251018_164510.json\"\n"
      ],
      "metadata": {
        "id": "JpjQT9DNTSVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfLz-PqQeoCO"
      },
      "source": [
        "### Evaluation before GRPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4_cxUamWbKp"
      },
      "outputs": [],
      "source": [
        "base_model = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "evaluate_model(base_model, num_samples=LIMIT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vxM1p70fulX"
      },
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 1: </b><br>\n",
        "\n",
        "- What are the main conceptual and practical differences between **Direct Preference Optimization (DPO)** and **Group Relative Policy Optimization (GRPO)**?  \n",
        "- In what situations might one method outperform the other?  \n",
        "- How do both approaches approximate the **KL-regularized RL objective** derived in [Christiano et al., 2017](https://arxiv.org/abs/1706.03741)?\n",
        "\n",
        "üìñ *References:*  \n",
        "- Rafailov et al., 2023. *Direct Preference Optimization: Your Language Model is Secretly a Reward Model.* [arXiv:2305.18290](https://arxiv.org/abs/2305.18290)\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DJq0nc5gOmL"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Answer 1: </b><br>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. GRPO overview**\n",
        "\n",
        "**GRPO** is one of the existing methods for fine-tuning language models with human feedback (RLHF).\n",
        "\n",
        "* Reward moddeling phase:  \n",
        "First, it fits a reward model $r_œï$ to a dataset of prompts and\n",
        "human preferences over pairs of responses. To do so, it uses a preference model such as the [Bradley‚ÄìTerry model](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model#math_1).\n",
        "\n",
        "* RL Fine-Tuning Phase:  \n",
        "Then, it uses RL to find a policy that maximizes the learned reward. This phase aims to solve the following optimization problem: ![GRPO Training Overview](https://1drv.ms/i/c/ae69638675180117/IQQ-KizPdUxCRZU9qDGcpX1AAeettH1uhsJqqM1WjXiYR6s?width=705&height=66)\n",
        "\n",
        "\n",
        "\n",
        "Conceptually it is close to RLHF with PPO but removes the critic and replaces advantage estimation with relative scoring across the groups.  \n",
        "\n",
        "Practically, GRPO requires on-the-fly generation and a reward function, is more compute-intensive and potentially less stable, but it is flexible: rewards can be continuous, automatically computed, and adapted during training. Nevertheless, **fine-tuning large language models with reinforcement learning remains a major practical challenge** and that lead to approaches optimizing relative preferences without RL, such as **DPO**."
      ],
      "metadata": {
        "id": "gBmOOy1bNYTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**2. DPO overview**\n",
        "\n",
        "Unlike prior RLHF methods, and in particular **GRPO**, which learn a reward and then optimize it via RL, **DPO** leverages a particular choice of reward model parameterization that enables extraction of its optimal policy in closed form, **without an RL training loop**.\n",
        "![DPO](https://1drv.ms/i/c/ae69638675180117/IQQ_wS77RKdrS4tzLbwoqr1gAR0Bf_1X2U36NRBp1Odsypg?width=560&height=48)     (1)\n",
        "\n",
        "\n",
        "Theoretically, it also relies on a [Bradley‚ÄìTerry model](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model#math_1) linking preferences to an implicit reward, practically, it is **simple, stable, data-efficient, and cheap to run**, but it is limited to pairwise supervision and cannot directly exploit graded or task-specific, automatically computable rewards."
      ],
      "metadata": {
        "id": "qAzxeJHXMHnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. In what situations might one method outperform the other?**  \n",
        "  \n",
        "**DPO** is used when you have large, clean pairwise-preference datasets and limited compute, or when you want stable alignment with minimal policy drift. Also, it often when robustness and simplicity are key.\n",
        "\n",
        "**GRPO** is used when you can define reliable automatic rewards and need online improvement‚Äîmath/code/format-sensitive tasks, safety shaping with dense signals, non-binary preferences, or domains where exploration matters and targets evolve. Moreover, it outperforms **DPO** on verifiable tasks with informative rewards or when long, diverse generations benefit from relative scoring."
      ],
      "metadata": {
        "id": "Bl0qhU2yOMol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Relation to the KL-regularized RL objective based on (Christiano et al., 2017) for each method.**\n",
        "\n",
        "The KL-regularized RL objective is:\n",
        "\n",
        "$\\displaystyle \\max_{\\pi_\\theta}\\mathbb{E}_{x\\sim D, y\\sim \\pi_\\theta(\\cdot\\mid x)}\\big[r(x,y)\\big]-\\beta \\ \\mathrm{KL}\\big(\\pi_\\theta(\\cdot\\mid x)\\ ||\\  \\pi_{\\text{ref}}(\\cdot\\mid x)\\big).$  \n",
        "\n",
        "For **DPO**, starting from this RL objective, it is shown in [Rafailov et al., 2023.](https://arxiv.org/pdf/2305.18290#page=14&zoom=100,144,905) that the optimal solution is given in closed form by:\n",
        "\n",
        "$$\n",
        "\\pi_r(y \\mid x)\n",
        "= \\frac{1}{Z(x)}\\, \\pi_{\\text{ref}}(y \\mid x)\\, \\exp\\!\\left(\\frac{1}{\\beta}\\, r(x,y)\\right)\n",
        "$$\n",
        "where, $Z(x) = \\sum_{y} \\pi_{\\text{ref}}(y \\mid x)\\,\\exp\\!\\left(\\frac{1}{\\beta}\\, r(x,y)\\right)$\n",
        "\n",
        "This gives an *implicit reward* $\n",
        "\\ r(x,y) = \\beta \\log \\frac{\\pi_r(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + \\beta \\log Z(x)\n",
        "$\n",
        ", and after some steps a final loss:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta;\\,\\pi_{\\text{ref}})\n",
        "= -\\,\\mathbb{E}_{(x,y_w,y_l)\\sim \\mathcal{D}}\n",
        "\\bigg[\n",
        "\\log \\sigma\\!\\Big(\n",
        "\\beta \\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\text{ref}}(y_w \\mid x)}\n",
        "-\\beta \\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\text{ref}}(y_l \\mid x)}\n",
        "\\Big)\n",
        "\\bigg].\n",
        "$$  \n",
        "\n",
        "  \n",
        "\n",
        "In the other hand, **GRPO** directly forms a sampled estimator of the same objective by weighting $\\nabla_\\theta \\log \\pi_\\theta(y_i\\mid x)$ with softmax-normalized $r(x,y_i)$ across group samples and adding an explicit $\\beta \\ \\mathrm{KL}\\big(\\pi_\\theta \\ || \\ \\pi_{\\text{ref}}\\big)$ regularizer.  \n",
        "\n",
        "![GRPO Training Overview](https://1drv.ms/i/c/ae69638675180117/IQQ-KizPdUxCRZU9qDGcpX1AAeettH1uhsJqqM1WjXiYR6s?width=705&height=66)"
      ],
      "metadata": {
        "id": "it_Sg5tENDzQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10CojP_of_MM"
      },
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 2: </b><br>\n",
        "\n",
        "- Why can **reward model overfitting** or **reward hacking** occur in reinforcement fine-tuning?  \n",
        "- How do DPO and GRPO attempt to mitigate this without explicit online reward models?  \n",
        "- Discuss the role of the **reference model** in maintaining stability.\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPsmM7wOgPsQ"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Answer 2: </b><br>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Why reward model overfitting / reward hacking occurs.**  \n",
        "\n",
        "In **RLHF** and in particular in **GRPO**, the reward model is trained on a small, biased set of preferences, so it can learn shallow cues instead of true quality. During RL, the policy explores regions the reward model never saw and finds blind spots that give high scores without better answers. Sparse or noisy rewards and weak KL control make this easier, and GRPO‚Äôs generation can amplify it by probing those blind spots faster."
      ],
      "metadata": {
        "id": "F6P-dCZuWpDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. How DPO and GRPO mitigate this without explicit online reward models.**  \n",
        "\n",
        "**DPO** avoids an online reward model. It turns pairwise preferences into a closed-form loss that compares the policy to a fixed reference (1). This directly optimizes relative preference gaps and keeps the policy anchored by a KL term, which limits drift into regions where a learned reward would be unreliable.\n",
        "\n",
        "**GRPO** reduces reliance on a fragile reward by using group-relative scoring instead of absolute scores, plus a KL to a reference to cap distribution shift. It can use simple, automatic rewards and early stopping, which lowers the chance of chasing spurious patterns that a learned reward would amplify."
      ],
      "metadata": {
        "id": "GT-0nLz9Wwfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Role of the reference model in maintaining stability.**  \n",
        "\n",
        "In **GRPO**, the reference policy $\\pi_{\\text{ref}}$ is an anchor that enforces a trust region via an explicit or implicit KL term, typically maximizing $-\\beta \\ \\mathrm{KL}\\big(\\pi_\\theta(\\cdot\\mid x) \\ ||\\ \\pi_{\\text{ref}}(\\cdot\\mid x)\\big)$ so updates keep $\\pi_\\theta$ in a high-likelihood neighborhood of a well-behaved base model.  \n",
        "\n",
        "In **DPO**, $\\pi_{\\text{ref}}$ appears inside the log-ratio that defines the preference margin, acting as a built-in KL regularizer:\n",
        "$$\n",
        "\\max_{\\pi_\\theta}\\;\n",
        "\\mathbb{E}_{y\\sim \\pi_\\theta(y\\mid x)}\n",
        "\\Bigg[{r_\\phi(x,y)\n",
        "-\\beta \\log \\sum_{y} \\pi_{\\text{ref}}(y\\mid x)\\,\n",
        "\\exp\\!\\left(\\frac{1}{\\beta}\\, r_\\phi(x,y)\\right)}\n",
        "\\;-\\;\n",
        "\\underbrace{\\beta \\log \\frac{\\pi_\\theta(y\\mid x)}{\\pi_{\\text{ref}}(y\\mid x)}}_{\\text{KL}}\n",
        "\\Bigg]\n",
        "$$\n",
        "\n",
        "The hyperparameter $\\beta$ sets the trade-off between following the reward and staying close to the reference. Small $\\beta$ means a strong KL pull to the reference, **safer and more stable but conservative and slower to improve.** Large $\\beta$ weakens the KL, the model chases reward more, **learns faster, but risks drift and reward hacking.**"
      ],
      "metadata": {
        "id": "9ea7Ay9FW9by"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6NpkzPMgJh-"
      },
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 3: </b><br>\n",
        "\n",
        "- In the GSM8K setup, we used a simple binary reward for **format** and **correctness**.  \n",
        "  - What are the limitations of such sparse rewards?  \n",
        "  - How could you design a **richer, smoother** reward signal for math reasoning?\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAl8OLepgRDO"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Answer 3: </b><br>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Limiations of such sparse rewards**\n",
        "1. No gradient for ‚Äúnear-misses‚Äù: A response that is numerically off by 1, or perfectly reasoned but with a tiny parsing mistake, gets the same 0 as a random guess, which is poor credit assignment and inefficient exploration.\n",
        "\n",
        "2. High variance and slow: since many trials give no gradient, so training is noisy.\n",
        "\n",
        "3. Easy to game: models learn surface patterns of format without better math.\n",
        "\n",
        "4. Poor transfer: gains do not generalize to new prompts or longer chains."
      ],
      "metadata": {
        "id": "DLNM7zBVwSNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Possible solutions**\n",
        "\n",
        "1. Softer answer rewards.  \n",
        "Use a proximity score $r_{\\text{num}} = \\exp\\!\\big(-|y - \\hat{y}| / \\tau\\big)$ or a scaled error after canonicalizing numeric forms.\n",
        "\n",
        "2. Smooth format reward.\n",
        "Score the final line with edit distance, for example:\n",
        "$$r_{\\text{fmt}} = 1 - \\min\\!\\left(1,\\; \\frac{\\mathrm{ED}(\\text{last\\_line}, \\langle\\text{answer}\\rangle \\hat{y} \\langle/\\text{answer}\\rangle)}{L}\\right)$$\n",
        "where $\\text{ED}$ is the [Levenshtein distance](https://fr.wikipedia.org/wiki/Distance_de_Levenshtein), and $L$ is a normalizing constant to stay in $[0, 1]$\n",
        "\n",
        "3. Stepwise reasoning.  \n",
        "Verify operations and set $r_{\\text{steps}} = \\frac{\\#\\text{verified}}{\\#\\text{parsed}}$  \n",
        "**Example :**  \n",
        "**From** $7\\times 8=56$\n",
        "   - $A = 7\\times 8$\n",
        "   - $B = 56$\n",
        "   - Evaluate: $A=56$, $B=56$ -> correct  \n",
        "   \n",
        "   **From** $56+5=56$\n",
        "   - $A = 56+5$\n",
        "   - $B = 56$  \n",
        "   - Evaluate: $A=61$, $B=65$ -> wrong\n",
        "\n",
        "   So, $r_\\text{steps}= \\frac{1}{2}$\n",
        "\n",
        "\n",
        "Thus, we can combine those rewards and get :   \n",
        "$\n",
        "r = 0.4\\,r_{\\text{num}} + 0.3\\,r_{\\text{fmt}} + 0.3\\,r_{\\text{steps}}$ and $ r = \\text{clip}(r, 0, 1)$\n",
        "\n"
      ],
      "metadata": {
        "id": "KZJQ2AO3zjJe"
      }
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}